{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "def split_dir_to_train_test_val(directory = \"data/\",\n",
    "                            train_size = 0.7,\n",
    "                            test_size = 0.2,\n",
    "                            val_size = 0.1):\n",
    "  \"\"\"\n",
    "  Creates 3 folders for Train, Test and Validation data\n",
    "  \"\"\"\n",
    "  import os\n",
    "  import random\n",
    "  import shutil\n",
    "\n",
    "  # Set random seed\n",
    "  rng = random.Random(42)\n",
    "\n",
    "  for root, folders, files in os.walk(directory):\n",
    "    for folder in folders:\n",
    "      # Create list of the files\n",
    "      list_of_files = []\n",
    "      for file_name in os.listdir(root+folder+\"/\"):\n",
    "        list_of_files.append(file_name)\n",
    "      \n",
    "      #  Shuffle the list\n",
    "      rng.shuffle(list_of_files)\n",
    "\n",
    "      # Create lists of files\n",
    "      train_files = list_of_files[:int(len(list_of_files)*train_size)]\n",
    "      test_files = list_of_files[int(len(list_of_files)*train_size) : int(len(list_of_files)*(train_size+test_size))]\n",
    "      val_files = list_of_files[int(len(list_of_files)*(train_size+test_size)):]\n",
    "\n",
    "      # Create folders and files for train data\n",
    "      for one_file in train_files:\n",
    "      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/train/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Train data copied. {len(train_files)} files\")\n",
    "\n",
    "      # Create folders and files for test data\n",
    "      for one_file in test_files:      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/test/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Test data copied. {len(test_files)} files\")\n",
    "\n",
    "      # Create folders and files for validation data\n",
    "      for one_file in val_files:\n",
    "      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/validation/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Validation data copied. {len(val_files)} files\")\n",
    "      \n",
    "     \n",
    "\n",
    "\n",
    "def get_class_names_from_folder(directory):\n",
    "  \"\"\"\n",
    "  Get the classnames from train folder for example\n",
    "  \"\"\"\n",
    "  import pathlib\n",
    "  import numpy as np\n",
    "  data_dir = pathlib.Path(directory)\n",
    "  class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # Created a list of class names \n",
    "  return class_names\n",
    "  print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder kulfi. Train data copied. 104 files\n",
      "Folder kulfi. Test data copied. 30 files\n",
      "Folder kulfi. Validation data copied. 0 files\n",
      "Folder jalebi. Train data copied. 151 files\n",
      "Folder jalebi. Test data copied. 43 files\n",
      "Folder jalebi. Validation data copied. 0 files\n",
      "Folder burger. Train data copied. 181 files\n",
      "Folder burger. Test data copied. 52 files\n",
      "Folder burger. Validation data copied. 0 files\n",
      "Folder pizza. Train data copied. 143 files\n",
      "Folder pizza. Test data copied. 41 files\n",
      "Folder pizza. Validation data copied. 0 files\n",
      "Folder kadai_paneer. Train data copied. 177 files\n",
      "Folder kadai_paneer. Test data copied. 51 files\n",
      "Folder kadai_paneer. Validation data copied. 0 files\n",
      "Folder pakode. Train data copied. 139 files\n",
      "Folder pakode. Test data copied. 40 files\n",
      "Folder pakode. Validation data copied. 0 files\n",
      "Folder butter_naan. Train data copied. 169 files\n",
      "Folder butter_naan. Test data copied. 48 files\n",
      "Folder butter_naan. Validation data copied. 0 files\n",
      "Folder pav_bhaji. Train data copied. 163 files\n",
      "Folder pav_bhaji. Test data copied. 47 files\n",
      "Folder pav_bhaji. Validation data copied. 0 files\n",
      "Folder momos. Train data copied. 175 files\n",
      "Folder momos. Test data copied. 50 files\n",
      "Folder momos. Validation data copied. 0 files\n",
      "Folder fried_rice. Train data copied. 195 files\n",
      "Folder fried_rice. Test data copied. 55 files\n",
      "Folder fried_rice. Validation data copied. 0 files\n",
      "Folder dhokla. Train data copied. 130 files\n",
      "Folder dhokla. Test data copied. 37 files\n",
      "Folder dhokla. Validation data copied. 0 files\n",
      "Folder kaathi_rolls. Train data copied. 148 files\n",
      "Folder kaathi_rolls. Test data copied. 43 files\n",
      "Folder kaathi_rolls. Validation data copied. 0 files\n",
      "Folder paani_puri. Train data copied. 66 files\n",
      "Folder paani_puri. Test data copied. 19 files\n",
      "Folder paani_puri. Validation data copied. 0 files\n",
      "Folder masala_dosa. Train data copied. 145 files\n",
      "Folder masala_dosa. Test data copied. 42 files\n",
      "Folder masala_dosa. Validation data copied. 0 files\n",
      "Folder chai. Train data copied. 192 files\n",
      "Folder chai. Test data copied. 55 files\n",
      "Folder chai. Validation data copied. 0 files\n",
      "Folder samosa. Train data copied. 127 files\n",
      "Folder samosa. Test data copied. 37 files\n",
      "Folder samosa. Validation data copied. 0 files\n",
      "Folder dal_makhani. Train data copied. 158 files\n",
      "Folder dal_makhani. Test data copied. 45 files\n",
      "Folder dal_makhani. Validation data copied. 0 files\n",
      "Folder chole_bhature. Train data copied. 203 files\n",
      "Folder chole_bhature. Test data copied. 58 files\n",
      "Folder chole_bhature. Validation data copied. 0 files\n",
      "Folder idli. Train data copied. 161 files\n",
      "Folder idli. Test data copied. 46 files\n",
      "Folder idli. Validation data copied. 0 files\n",
      "Folder chapati. Train data copied. 179 files\n",
      "Folder chapati. Test data copied. 51 files\n",
      "Folder chapati. Validation data copied. 0 files\n"
     ]
    }
   ],
   "source": [
    "# Split images dir to train, test and validation\n",
    "split_dir_to_train_test_val(directory=\"data/\",\n",
    "                            train_size=0.78,\n",
    "                            test_size=0.22,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3106 files belonging to 20 classes.\n",
      "Found 890 files belonging to 20 classes.\n",
      "['burger', 'butter_naan', 'chai', 'chapati', 'chole_bhature', 'dal_makhani', 'dhokla', 'fried_rice', 'idli', 'jalebi', 'kaathi_rolls', 'kadai_paneer', 'kulfi', 'masala_dosa', 'momos', 'paani_puri', 'pakode', 'pav_bhaji', 'pizza', 'samosa']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_path = \"files/train/\"\n",
    "test_path = \"files/test/\"\n",
    "\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "data_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    shuffle=True,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=32)\n",
    "\n",
    "data_categs = data_train.class_names\n",
    "\n",
    "data_val = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_path,\n",
    "    shuffle=True,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=32)\n",
    "\n",
    "\n",
    "print(data_categs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(units=len(data_categs)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 345ms/step - accuracy: 0.1086 - loss: 2.8743 - val_accuracy: 0.2506 - val_loss: 2.3964\n",
      "Epoch 2/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 418ms/step - accuracy: 0.3352 - loss: 2.1758 - val_accuracy: 0.2775 - val_loss: 2.5480\n",
      "Epoch 3/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 375ms/step - accuracy: 0.4692 - loss: 1.7491 - val_accuracy: 0.3888 - val_loss: 2.1021\n",
      "Epoch 4/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 377ms/step - accuracy: 0.6485 - loss: 1.1628 - val_accuracy: 0.3247 - val_loss: 2.6038\n",
      "Epoch 5/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 375ms/step - accuracy: 0.7946 - loss: 0.7158 - val_accuracy: 0.3854 - val_loss: 2.7923\n",
      "Epoch 6/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 385ms/step - accuracy: 0.9276 - loss: 0.2599 - val_accuracy: 0.3416 - val_loss: 3.2084\n",
      "Epoch 7/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 377ms/step - accuracy: 0.9688 - loss: 0.1138 - val_accuracy: 0.3843 - val_loss: 3.6310\n",
      "Epoch 8/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 343ms/step - accuracy: 0.9605 - loss: 0.1292 - val_accuracy: 0.3753 - val_loss: 3.5676\n",
      "Epoch 9/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 395ms/step - accuracy: 0.9934 - loss: 0.0335 - val_accuracy: 0.3685 - val_loss: 4.1059\n",
      "Epoch 10/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 390ms/step - accuracy: 0.9934 - loss: 0.0284 - val_accuracy: 0.3798 - val_loss: 4.1458\n",
      "Epoch 11/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 306ms/step - accuracy: 0.9965 - loss: 0.0190 - val_accuracy: 0.3551 - val_loss: 4.9221\n",
      "Epoch 12/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 316ms/step - accuracy: 0.9240 - loss: 0.2994 - val_accuracy: 0.3607 - val_loss: 4.3096\n",
      "Epoch 13/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 314ms/step - accuracy: 0.9775 - loss: 0.0787 - val_accuracy: 0.3596 - val_loss: 4.3528\n",
      "Epoch 14/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 311ms/step - accuracy: 0.9961 - loss: 0.0164 - val_accuracy: 0.3876 - val_loss: 4.3620\n",
      "Epoch 15/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 316ms/step - accuracy: 0.9995 - loss: 0.0050 - val_accuracy: 0.3809 - val_loss: 4.6163\n",
      "Epoch 16/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 367ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.3933 - val_loss: 4.6651\n",
      "Epoch 17/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.3989 - val_loss: 4.8283\n",
      "Epoch 18/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 378ms/step - accuracy: 1.0000 - loss: 6.2364e-04 - val_accuracy: 0.3966 - val_loss: 4.8831\n",
      "Epoch 19/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 375ms/step - accuracy: 1.0000 - loss: 4.4134e-04 - val_accuracy: 0.4000 - val_loss: 4.9358\n",
      "Epoch 20/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 371ms/step - accuracy: 1.0000 - loss: 3.9834e-04 - val_accuracy: 0.3989 - val_loss: 5.0037\n",
      "Epoch 21/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 372ms/step - accuracy: 1.0000 - loss: 3.8779e-04 - val_accuracy: 0.4000 - val_loss: 5.0501\n",
      "Epoch 22/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 3.3325e-04 - val_accuracy: 0.3933 - val_loss: 5.1746\n",
      "Epoch 23/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 4.9884e-04 - val_accuracy: 0.3910 - val_loss: 5.1635\n",
      "Epoch 24/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 2.4688e-04 - val_accuracy: 0.3966 - val_loss: 5.2139\n",
      "Epoch 25/25\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 337ms/step - accuracy: 1.0000 - loss: 1.7592e-04 - val_accuracy: 0.3966 - val_loss: 5.2643\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    data_train,\n",
    "    validation_data=data_val,\n",
    "    epochs=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ind.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx1n3gp54/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx1n3gp54/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpx1n3gp54'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name='keras_tensor_11')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 20), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  123729091001424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729091000656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729090995280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729091002192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729090999120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729091000464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729089855952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  123729089856912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1725695513.539613    5597 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1725695513.540443    5597 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-07 13:21:53.543890: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpx1n3gp54\n",
      "2024-09-07 13:21:53.544642: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-07 13:21:53.544662: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpx1n3gp54\n",
      "2024-09-07 13:21:53.556238: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-07 13:21:53.599387: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpx1n3gp54\n",
      "2024-09-07 13:21:53.613564: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 69900 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('ind.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burger': 0,\n",
       " 'butter_naan': 1,\n",
       " 'chai': 2,\n",
       " 'chapati': 3,\n",
       " 'chole_bhature': 4,\n",
       " 'dal_makhani': 5,\n",
       " 'dhokla': 6,\n",
       " 'fried_rice': 7,\n",
       " 'idli': 8,\n",
       " 'jalebi': 9,\n",
       " 'kaathi_rolls': 10,\n",
       " 'kadai_paneer': 11,\n",
       " 'kulfi': 12,\n",
       " 'masala_dosa': 13,\n",
       " 'momos': 14,\n",
       " 'paani_puri': 15,\n",
       " 'pakode': 16,\n",
       " 'pav_bhaji': 17,\n",
       " 'pizza': 18,\n",
       " 'samosa': 19}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
