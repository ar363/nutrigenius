{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "def split_dir_to_train_test_val(directory = \"data/\",\n",
    "                            train_size = 0.7,\n",
    "                            test_size = 0.2,\n",
    "                            val_size = 0.1):\n",
    "  \"\"\"\n",
    "  Creates 3 folders for Train, Test and Validation data\n",
    "  \"\"\"\n",
    "  import os\n",
    "  import random\n",
    "  import shutil\n",
    "\n",
    "  # Set random seed\n",
    "  rng = random.Random(42)\n",
    "\n",
    "  for root, folders, files in os.walk(directory):\n",
    "    for folder in folders:\n",
    "      # Create list of the files\n",
    "      list_of_files = []\n",
    "      for file_name in os.listdir(root+folder+\"/\"):\n",
    "        list_of_files.append(file_name)\n",
    "      \n",
    "      #  Shuffle the list\n",
    "      rng.shuffle(list_of_files)\n",
    "\n",
    "      # Create lists of files\n",
    "      train_files = list_of_files[:int(len(list_of_files)*train_size)]\n",
    "      test_files = list_of_files[int(len(list_of_files)*train_size) : int(len(list_of_files)*(train_size+test_size))]\n",
    "      val_files = list_of_files[int(len(list_of_files)*(train_size+test_size)):]\n",
    "\n",
    "      # Create folders and files for train data\n",
    "      for one_file in train_files:\n",
    "      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/train/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Train data copied. {len(train_files)} files\")\n",
    "\n",
    "      # Create folders and files for test data\n",
    "      for one_file in test_files:      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/test/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Test data copied. {len(test_files)} files\")\n",
    "\n",
    "      # Create folders and files for validation data\n",
    "      for one_file in val_files:\n",
    "      \n",
    "        # Copy  files\n",
    "        dest_dir = \"files/validation/\"+folder+\"/\"\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        shutil.copy2(src=(root+folder+\"/\"+one_file),\n",
    "                    dst=(dest_dir+one_file))\n",
    "      print(f\"Folder {folder}. Validation data copied. {len(val_files)} files\")\n",
    "      \n",
    "     \n",
    "\n",
    "\n",
    "def get_class_names_from_folder(directory):\n",
    "  \"\"\"\n",
    "  Get the classnames from train folder for example\n",
    "  \"\"\"\n",
    "  import pathlib\n",
    "  import numpy as np\n",
    "  data_dir = pathlib.Path(directory)\n",
    "  class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # Created a list of class names \n",
    "  return class_names\n",
    "  print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder kulfi. Train data copied. 93 files\n",
      "Folder kulfi. Test data copied. 27 files\n",
      "Folder kulfi. Validation data copied. 14 files\n",
      "Folder jalebi. Train data copied. 135 files\n",
      "Folder jalebi. Test data copied. 39 files\n",
      "Folder jalebi. Validation data copied. 20 files\n",
      "Folder burger. Train data copied. 163 files\n",
      "Folder burger. Test data copied. 46 files\n",
      "Folder burger. Validation data copied. 24 files\n",
      "Folder pizza. Train data copied. 128 files\n",
      "Folder pizza. Test data copied. 37 files\n",
      "Folder pizza. Validation data copied. 19 files\n",
      "Folder kadai_paneer. Train data copied. 159 files\n",
      "Folder kadai_paneer. Test data copied. 46 files\n",
      "Folder kadai_paneer. Validation data copied. 23 files\n",
      "Folder pakode. Train data copied. 125 files\n",
      "Folder pakode. Test data copied. 36 files\n",
      "Folder pakode. Validation data copied. 18 files\n",
      "Folder butter_naan. Train data copied. 151 files\n",
      "Folder butter_naan. Test data copied. 44 files\n",
      "Folder butter_naan. Validation data copied. 22 files\n",
      "Folder pav_bhaji. Train data copied. 147 files\n",
      "Folder pav_bhaji. Test data copied. 41 files\n",
      "Folder pav_bhaji. Validation data copied. 22 files\n",
      "Folder momos. Train data copied. 157 files\n",
      "Folder momos. Test data copied. 45 files\n",
      "Folder momos. Validation data copied. 23 files\n",
      "Folder fried_rice. Train data copied. 175 files\n",
      "Folder fried_rice. Test data copied. 49 files\n",
      "Folder fried_rice. Validation data copied. 26 files\n",
      "Folder dhokla. Train data copied. 116 files\n",
      "Folder dhokla. Test data copied. 34 files\n",
      "Folder dhokla. Validation data copied. 17 files\n",
      "Folder kaathi_rolls. Train data copied. 133 files\n",
      "Folder kaathi_rolls. Test data copied. 38 files\n",
      "Folder kaathi_rolls. Validation data copied. 20 files\n",
      "Folder paani_puri. Train data copied. 59 files\n",
      "Folder paani_puri. Test data copied. 17 files\n",
      "Folder paani_puri. Validation data copied. 9 files\n",
      "Folder masala_dosa. Train data copied. 130 files\n",
      "Folder masala_dosa. Test data copied. 38 files\n",
      "Folder masala_dosa. Validation data copied. 19 files\n",
      "Folder chai. Train data copied. 172 files\n",
      "Folder chai. Test data copied. 50 files\n",
      "Folder chai. Validation data copied. 25 files\n",
      "Folder samosa. Train data copied. 114 files\n",
      "Folder samosa. Test data copied. 33 files\n",
      "Folder samosa. Validation data copied. 17 files\n",
      "Folder dal_makhani. Train data copied. 142 files\n",
      "Folder dal_makhani. Test data copied. 40 files\n",
      "Folder dal_makhani. Validation data copied. 21 files\n",
      "Folder chole_bhature. Train data copied. 182 files\n",
      "Folder chole_bhature. Test data copied. 52 files\n",
      "Folder chole_bhature. Validation data copied. 27 files\n",
      "Folder idli. Train data copied. 144 files\n",
      "Folder idli. Test data copied. 42 files\n",
      "Folder idli. Validation data copied. 21 files\n",
      "Folder chapati. Train data copied. 161 files\n",
      "Folder chapati. Test data copied. 45 files\n",
      "Folder chapati. Validation data copied. 24 files\n"
     ]
    }
   ],
   "source": [
    "# Split images dir to train, test and validation\n",
    "split_dir_to_train_test_val(directory=\"data/\",\n",
    "                            train_size=0.7,\n",
    "                            test_size=0.2,\n",
    "                            val_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['burger', 'butter_naan', 'chai', 'chapati', 'chole_bhature',\n",
       "       'dal_makhani', 'dhokla', 'fried_rice', 'idli', 'jalebi',\n",
       "       'kaathi_rolls', 'kadai_paneer', 'kulfi', 'masala_dosa', 'momos',\n",
       "       'paani_puri', 'pakode', 'pav_bhaji', 'pizza', 'samosa'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using helper function get class names\n",
    "class_names = get_class_names_from_folder(directory=\"files/train/\")\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2786 files belonging to 20 classes.\n",
      "Found 799 files belonging to 20 classes.\n",
      "Found 411 files belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_path = \"files/train/\"\n",
    "test_path = \"files/test/\"\n",
    "validation_path = \"files/validation/\"\n",
    "\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "data_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    shuffle=True,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=32)\n",
    "\n",
    "data_categs = data_train.class_names\n",
    "\n",
    "data_test = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_path,\n",
    "    shuffle=True,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=32)\n",
    "\n",
    "data_val = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_path,\n",
    "    shuffle=True,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(units=len(data_categs)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 329ms/step - accuracy: 0.0896 - loss: 2.9375 - val_accuracy: 0.1922 - val_loss: 2.6194\n",
      "Epoch 2/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 290ms/step - accuracy: 0.2646 - loss: 2.3603 - val_accuracy: 0.2822 - val_loss: 2.3728\n",
      "Epoch 3/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 281ms/step - accuracy: 0.4503 - loss: 1.8293 - val_accuracy: 0.3431 - val_loss: 2.4000\n",
      "Epoch 4/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 287ms/step - accuracy: 0.6751 - loss: 1.0728 - val_accuracy: 0.2676 - val_loss: 2.8088\n",
      "Epoch 5/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 275ms/step - accuracy: 0.8119 - loss: 0.6638 - val_accuracy: 0.3041 - val_loss: 3.3286\n",
      "Epoch 6/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 284ms/step - accuracy: 0.9243 - loss: 0.2822 - val_accuracy: 0.3431 - val_loss: 4.1312\n",
      "Epoch 7/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 279ms/step - accuracy: 0.9699 - loss: 0.1165 - val_accuracy: 0.3285 - val_loss: 4.1244\n",
      "Epoch 8/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 286ms/step - accuracy: 0.9781 - loss: 0.0805 - val_accuracy: 0.3187 - val_loss: 4.3805\n",
      "Epoch 9/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 298ms/step - accuracy: 0.9902 - loss: 0.0380 - val_accuracy: 0.3431 - val_loss: 4.7039\n",
      "Epoch 10/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 274ms/step - accuracy: 0.9955 - loss: 0.0240 - val_accuracy: 0.3406 - val_loss: 5.1526\n",
      "Epoch 11/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 276ms/step - accuracy: 0.9937 - loss: 0.0254 - val_accuracy: 0.3333 - val_loss: 4.9974\n",
      "Epoch 12/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 283ms/step - accuracy: 0.9987 - loss: 0.0150 - val_accuracy: 0.3358 - val_loss: 5.1723\n",
      "Epoch 13/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 282ms/step - accuracy: 0.9947 - loss: 0.0272 - val_accuracy: 0.3139 - val_loss: 5.4590\n",
      "Epoch 14/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 281ms/step - accuracy: 0.9900 - loss: 0.0407 - val_accuracy: 0.3090 - val_loss: 5.5019\n",
      "Epoch 15/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 279ms/step - accuracy: 0.9849 - loss: 0.0579 - val_accuracy: 0.2822 - val_loss: 5.4276\n",
      "Epoch 16/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 276ms/step - accuracy: 0.9811 - loss: 0.0656 - val_accuracy: 0.3212 - val_loss: 5.7160\n",
      "Epoch 17/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 283ms/step - accuracy: 0.9963 - loss: 0.0152 - val_accuracy: 0.3260 - val_loss: 5.8697\n",
      "Epoch 18/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 289ms/step - accuracy: 0.9968 - loss: 0.0189 - val_accuracy: 0.3333 - val_loss: 6.0394\n",
      "Epoch 19/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 284ms/step - accuracy: 0.9986 - loss: 0.0107 - val_accuracy: 0.3309 - val_loss: 5.8692\n",
      "Epoch 20/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 296ms/step - accuracy: 0.9996 - loss: 0.0027 - val_accuracy: 0.3358 - val_loss: 6.2451\n",
      "Epoch 21/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 285ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.3479 - val_loss: 6.2195\n",
      "Epoch 22/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 288ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.3260 - val_loss: 6.4893\n",
      "Epoch 23/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 287ms/step - accuracy: 0.9973 - loss: 0.0076 - val_accuracy: 0.3066 - val_loss: 6.3730\n",
      "Epoch 24/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 284ms/step - accuracy: 0.9996 - loss: 0.0051 - val_accuracy: 0.3090 - val_loss: 6.7037\n",
      "Epoch 25/25\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 284ms/step - accuracy: 0.9974 - loss: 0.0082 - val_accuracy: 0.3236 - val_loss: 6.4163\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    data_train,\n",
    "    validation_data=data_val,\n",
    "    epochs=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ind.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpswg61num/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpswg61num/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpswg61num'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name='keras_tensor_39')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 20), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132121381266896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381259792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381263440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381268240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381265744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381268624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381265936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132121381269200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1725644326.463802   30021 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1725644326.464339   30021 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-06 23:08:46.470670: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpswg61num\n",
      "2024-09-06 23:08:46.471276: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-06 23:08:46.471296: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpswg61num\n",
      "2024-09-06 23:08:46.478152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-09-06 23:08:46.479064: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-06 23:08:46.534012: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpswg61num\n",
      "2024-09-06 23:08:46.544377: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 73692 microseconds.\n",
      "2024-09-06 23:08:46.590844: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('ind.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burger': 0,\n",
       " 'butter_naan': 1,\n",
       " 'chai': 2,\n",
       " 'chapati': 3,\n",
       " 'chole_bhature': 4,\n",
       " 'dal_makhani': 5,\n",
       " 'dhokla': 6,\n",
       " 'fried_rice': 7,\n",
       " 'idli': 8,\n",
       " 'jalebi': 9,\n",
       " 'kaathi_rolls': 10,\n",
       " 'kadai_paneer': 11,\n",
       " 'kulfi': 12,\n",
       " 'masala_dosa': 13,\n",
       " 'momos': 14,\n",
       " 'paani_puri': 15,\n",
       " 'pakode': 16,\n",
       " 'pav_bhaji': 17,\n",
       " 'pizza': 18,\n",
       " 'samosa': 19}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
